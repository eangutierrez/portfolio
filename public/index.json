[{"content":"🔗 GitHub Project Overview Created a MySQL script to upload and pre-process the data, as well as build several pivot tables to help leadership better understand customer segments. Created an R programming language script to upload and pre-process the data, build several pivot tables, and create visualizations to better present data to leadership. Created a Tableau Storyboard to present findings to leadership, including several recommendations. Wrote a series of Medium articles to futher explain each stage of the data analysis process. Code and Resources Used R Version: 4.2.2\nMySQL Version: 8.0.31\nData Cleaning After uploading the data, I performed several pre-processing tasks to prepare it for analysis. I performed the following changes to the data:\nAdded ride_length column by subtracting starting time from ending time day_of_week column by WEEKDAY() function Changed Renamed all csv files to follow \u0026ldquo;Bike__Data_YYYYMM\u0026rdquo; nomenclature Renamed \u0026ldquo;rideable_type\u0026rdquo; column to \u0026ldquo;bike_type\u0026rdquo; Renamed \u0026ldquo;start_station_name\u0026rdquo; column to \u0026ldquo;start_sta_name\u0026rdquo; Renamed \u0026ldquo;start_station_id\u0026rdquo; column to \u0026ldquo;start_sta_id\u0026rdquo; Renamed \u0026ldquo;end_station_name\u0026rdquo; column to \u0026ldquo;end_sta_name\u0026rdquo; Renamed \u0026ldquo;end_station_id\u0026rdquo; column to \u0026ldquo;end_sta_id\u0026rdquo; Renamed \u0026ldquo;member_casual\u0026rdquo; column to \u0026ldquo;user_type\u0026rdquo; column Renamed \u0026ldquo;rideable_type\u0026rdquo; column to \u0026ldquo;bike_type\u0026rdquo; column \u0026ldquo;started_at\u0026rdquo; column\u0026rsquo;s data type changed to date and time \u0026ldquo;ended_at\u0026rdquo; column\u0026rsquo;s data type changed to date and time Replaced \u0026ldquo;Clybourne Ave\u0026rdquo; to \u0026ldquo;Clybourn Ave\u0026rdquo; in all columns Applied the CamelCase naming convention to the data set Removed All null values in the data set All trailing, leading, and excess spaces in the data set All records with a ride length of less than one minute Exploratory Data Analysis I created several visualizations to better understand the data\u0026rsquo;s distribution, value differences, and categorical variables. Below are a few highlights from this phase:\n","permalink":"https://eangutierrez.github.io/portfolio/projects/project_1/","summary":"🔗 GitHub Project Overview Created a MySQL script to upload and pre-process the data, as well as build several pivot tables to help leadership better understand customer segments. Created an R programming language script to upload and pre-process the data, build several pivot tables, and create visualizations to better present data to leadership. Created a Tableau Storyboard to present findings to leadership, including several recommendations. Wrote a series of Medium articles to futher explain each stage of the data analysis process.","title":"Cyclistic Bike Share"},{"content":"Introduction In this series of articles, we will discuss the Cyclistic Bike-Share Business Case from the Google Data Analytics Professional Certificate.\nData analysis is a complicated process, so we must do our best to stay organized throughout the entire process. That is why we will divide this Case Study into six main sections, based on the certificate program’s Data Analysis Roadmap:\nAsk Prepare Process Analyze Share Act Data Analysis Roadmap Step 1: Ask The Ask Stage is where we hold deep discussions with all pertinent stakeholders to thoroughly understand our customers\u0026rsquo; needs. We must ask effective questions, define the customers\u0026rsquo; main problem, use structured thinking to plan a solution, and effectively communicate with others to ensure project success.\nBusiness Case Context In this scenario, we are data analysts working with Cyclistic, a bike-share company in Chicago. We are tasked with studying our users so we can provide data-driven recommendations to our leadership team.\nIn this section, we will go over the case’s main points. The first stage of the data analysis process is the Ask stage. From the initial discussions with the stakeholders, we need to define what the project will look like, what qualifies as a successful result, etc. You will find a summary of these discussions below.\nCyclistic Bike-Share is a fictitious company that offers 5824 bikes in 692 stations across Chicago to users for rent and use. There are two main users, casual riders, who purchase single-ride or full-day passes, and Cyclistic members, who purchase annual memberships.\nThe company’s finance analysist believe that annual memberships are much more profitable than casual rentals, so our marketing strategy should aim to maximize the number of annual members to consolidate future growth.\nAs data analysts on this company, our main task is to find out how do annual members and casual riders use Cyclistic bikes differently.\nOur main deliverables include:\n· A clear statement of the business task\n· A description of all data sources used\n· Documentation of any cleaning or manipulation of data\n· A summary of your analysis\n· Supporting visualizations and key findings\n· Our top three recommendations based on your analysis\nScope of Work The last component of the Ask stage is to create a Scope of Work document. A good Scope of Work document is catered to the specific project and sets expectations and boundaries of an agreed upon outline of the work you’re going to perform on a project. You will find a copy of this document below:\nConclusion With a thorough Scope of Work document, we are ready to move to the second stage of the data analysis process: the Prepare Stage.\nImage Credits Image Courtesy of: Oleksandr Canary Islands\n","permalink":"https://eangutierrez.github.io/portfolio/blog/part1/","summary":"Introduction In this series of articles, we will discuss the Cyclistic Bike-Share Business Case from the Google Data Analytics Professional Certificate.\nData analysis is a complicated process, so we must do our best to stay organized throughout the entire process. That is why we will divide this Case Study into six main sections, based on the certificate program’s Data Analysis Roadmap:\nAsk Prepare Process Analyze Share Act Data Analysis Roadmap Step 1: Ask The Ask Stage is where we hold deep discussions with all pertinent stakeholders to thoroughly understand our customers\u0026rsquo; needs.","title":"Cyclistic Bike-Share Business Case Part One: Introduction and Scope of Work"},{"content":"🔗 GitHub Project Overview Created a MySQL script to highlight some of the MySQL querying skills I learned while studying Learning SQL Third Edition by Alan Beaulieu. Set up a database and SQL editor to interact with the database, upload the schema, and dump tables. Practiced several intermediate and advanced SQL skills, including querying multiple tables, conditional logic, views, common table expressions, and analytic functions. Code and Resources Used MySQL Version: 8.0.31\nEntity Relationship Diagram Sample Tables ","permalink":"https://eangutierrez.github.io/portfolio/projects/project_2/","summary":"🔗 GitHub Project Overview Created a MySQL script to highlight some of the MySQL querying skills I learned while studying Learning SQL Third Edition by Alan Beaulieu. Set up a database and SQL editor to interact with the database, upload the schema, and dump tables. Practiced several intermediate and advanced SQL skills, including querying multiple tables, conditional logic, views, common table expressions, and analytic functions. Code and Resources Used MySQL Version: 8.","title":"Sakila Database Practice Queries"},{"content":"Data Analysis Roadmap Step 2: Prepare In the Prepare Stage, we must decide what data we need to collect in order to answer all stakeholder questions. We must also decide how to organize the data to maximize its utility. By focusing on what metrics to measure, as well as the security measures to implement, we will successfully complete this stage.\nCookiecutter Data Science Foldering Because all the data is provided for us, we should focus on the proper storage and security measures to implement. Using proper foldering techniques is important when working in teams, as this proper organization will minimize lost time. Using the Cookiecutter Data Science template on Github as a template, I created a Cyclistic_Bike_Share project folder with the following folders:\ndata (where the project’s data will be stored)\n- external (data found externally is stored here)\n- interim (data currently being analyzed is here)\n- processed (the fully cleaned data is here)\n- raw (the unprocessed data is here) docs models (any trained and serialized models are stored here) notebooks (any notebook files, such as Jupyter notebooks, are stored here) references (any data dictionaries, manual, and explanatory materials are stored here) reports (any HTML, PDF, or other generated analysis reports are stored here)\n- figures (any pgn, jpeg, or other data visualization files are stored here) scripts (any RStudio scripts are stored here) src (any source code for this project is stored here) Data Changelog File Another important aspect of the preparation stage is making a Data Changelog file. It is paramount to create and properly use a Data Changelog file, as this file is an explanation of all the notable changes to this data set. This file will be useful for our team to collaborate, keep track of all updates, and ensure all future insights are legitimate.\nBefore we look at the data, we must establish that the data’s integrity is intact. Only a data set that is reliable, original, comprehensive, current, and cited can be used for analysis, as it helps the client make business decisions based on accurate information. The original, current Cyclistic Bike-Share data was provided by the client, and since we cannot communicate with them to confirm that the data is free from sample biases that do not properly represent the population, human error, and other integral issues, I am making the assumption that the data is fit for analysis. In a real-life scenario, we would hold a meeting with the client to ensure that the data is credible and free of bias. To do so, we would go over the six main areas of data validation\nThe Six Pillars of Data Validation Data Type: ascertain the data matches the data type defined for each field Data Range: ascertain the data falls within an acceptable range of values defined for each field Data Constraints: ascertain the data meets proper input criteria Data Consistency: ascertain the data makes sense in the context of other related data Data Structure: ascertain the data follows a set structure Code Validation: ascertain the application code systematically performs all validations mentioned above. Client Privacy and Data License Agreement The Cyclistic Bike-Share data is available at: https://divvy-tripdata.s3.amazonaws.com/index.html. To protect the client’s privacy and security, we are saving and encrypting the documents on a password-protected computer. Additionally, we will delete any potential personal identifiable information so that customers are unidentifiable and remain anonymous. Finally, I will adhere to all the terms and conditions granted under this data set’s Data License Agreement.\nConclusion With all of these measures taken, we are ready to move to the third stage of the data analysis process: the Process Stage.\nReference: i Google Data Analytics Professional Certificate. (n.d.). Types of Data Validation. Coursera. [https://www.coursera.org/learn/analyze-data/supplement/tQAED/types-of-data-validation]\nImage Credits Image Courtesy of: Ian Livesey\n","permalink":"https://eangutierrez.github.io/portfolio/blog/part2/","summary":"Data Analysis Roadmap Step 2: Prepare In the Prepare Stage, we must decide what data we need to collect in order to answer all stakeholder questions. We must also decide how to organize the data to maximize its utility. By focusing on what metrics to measure, as well as the security measures to implement, we will successfully complete this stage.\nCookiecutter Data Science Foldering Because all the data is provided for us, we should focus on the proper storage and security measures to implement.","title":"Cyclistic Bike-Share Business Case Part Two: Preparation"},{"content":"Data Analysis Process Stage 3: Process Now that we have prepared, organized, and protected our data, we can move on to the Process Stage of the Data Analysis process. This stage is not only about cleaning the data. We must also verify and report on the cleaning results. This process of testing and transforming the data will help us maintain data integrity, thus ensuring that our results are valid.\nData Consolidation The 2022 data is divided into monthly increments. This means that each spreadsheet represents a month’s worth of bike-sharing data. I decided to create a uniform naming convention for each file, which is Bike_Data_YYYYMM. This will ease our calling of the data in Excel, Tableau, and RStudio.\nTo begin the analysis, I opened a month’s worth of data in Microsoft Excel. Excel is a good tool to perform an initial analysis and interpretation of the data set. Because one month contains over 100,000 tuples, however, it is possible that the consolidated data set will have over 1 million records. Excel will struggle to perform an analysis on such a robust data set. This is because Excel is designed to handle small to medium-sized data sets. For the sake of following this case study’s rules, however, I continued to use Excel to manipulate the data.\nAfter making sure that all sheets contained the same number of columns and the same labels, I combined all the 2022 monthly data sets into one by loading the entire interim data folder using the “Get Data From Folder” option in the Excel Data Tab. This will open the Power Query Editor, which will allow you to combine all 12 data sheets into one.\nData Preprocessing Now that the data is consolidated, we can begin the data preprocessing. I like to use the standard Google Data Cleanup Workflow. The 1st step is creating a copy of the data, so I unzipped the raw data files and loaded them into the interim data folder. The 2nd step is fixing the data table’s labels. I renamed all the labels that were inconsistent and ambiguous. One example of this is renaming the “rideable_type” column, which identifies whether a bike is electric, regular, or docked, into “bike_type” to make the label less obscure.\nThe 3rd step in the Data Cleanup Workflow, confirming missing data best practices, was difficult to complete. There were many nulls on the data set. In the real world, you would have a conversation with the client to clarify if this is a machine error during the data transfer, if this missing data can be recovered, or if this could be a dark data situation. Because this is a case study, however, I was forced to exclude all the null variables in the data set and only work with the completed data available. It is notable to say that some months had more missing data than others, so it would be interesting to contact the company to see what was the reason for this uneven distribution.\nThe 4th step is the clear formatting step, so I made sure the data set was organized in long format and compatible with database management systems, Tableau, RStudio, etc. The 5th step is confirming correct data types, so I looked at all the data types in each column to make sure that the spreadsheet correctly identified string, numeral, Boolean, and date types. The 6th step is the removing of duplicate records, so I used the standard “Group By” method in Power Query to find repeated records. The 7th step is removing irrelevant data, so I removed the ride_id column, which uniquely identified each ride. This column was removed because the data could potentially be used to identify customers. In the 8th step, I trimmed all trailing, leading, and excess spaces in the data cells.\nThe 9th step is to remove misspellings, so I performed a thorough search of the most commonly misspelled street names in the city. Finally, I made sure that all records in the dataset did not have inconsistent capitalization, incorrect punctuation, and other typos to complete the 12 stages of the Data Cleanup Workflow. The last task before analyzing the data is to record all these changes in the data changelog file.\nConclusion After completing our data changelog, we have completed the Process Stage. This is a big deal because we have done all the hard work to ensure our analysis process is easy, and more important, we have ensured the insights we find are legitimate. Now we are ready for the next step in the data analysis roadmap: the Analyze Stage.\n","permalink":"https://eangutierrez.github.io/portfolio/blog/part3/","summary":"Data Analysis Process Stage 3: Process Now that we have prepared, organized, and protected our data, we can move on to the Process Stage of the Data Analysis process. This stage is not only about cleaning the data. We must also verify and report on the cleaning results. This process of testing and transforming the data will help us maintain data integrity, thus ensuring that our results are valid.\nData Consolidation The 2022 data is divided into monthly increments.","title":"Cyclistic Bike-Share Business Case Part Three: Data Preprocessing"},{"content":"🔗 GitHub Project Overview Created several Python scripts I wrote while studying Python Crash Course Second Edition by Eric Matthes. Each folder contains the answers Python files for all practice problems. Practiced several Python skills, including data structures, functions, classes, APIs, and coding projects. Code and Resources Used Python Version: 3.10\n","permalink":"https://eangutierrez.github.io/portfolio/projects/project_3/","summary":"🔗 GitHub Project Overview Created several Python scripts I wrote while studying Python Crash Course Second Edition by Eric Matthes. Each folder contains the answers Python files for all practice problems. Practiced several Python skills, including data structures, functions, classes, APIs, and coding projects. Code and Resources Used Python Version: 3.10","title":"Python Crash Course Exercises"},{"content":"Data Analysis Process Stage 4: Analyze Now that we have uploaded, cleaned, and processed our data, we are ready to analyze and find key insights to help Cyclistic better understand their customers. In the Analyze stage, we use tools to format and transform, sort, and filter the data to find patterns and draw conclusions. We can make predictions and recommendation to help our stakeholders make data-informed decisions.\nPower Query Editor With a validated and clean data set in hand, we are ready to start the data analysis stage. For the sake of following this case study’s rules, I continued to use Excel’s Power Query Editor tool to manipulate the data.\nFirst, I added a column that contains the length of each ride, which was calculated by subtracting the ride’s start time from the end time. The ride_length column helped identify incoherent data records, such as trips with negative ride lengths, or canceled rides with a ride length of zero minutes. Without a client to discuss the best way to handle these records, I decided to filter them out of our analysis. Additionally, I created a column that states the day of the week the ride took place by using the Date.DayofWeek function. This was done to provide a deeper dimension to our analysis. This step concludes our work in the Power Query Editor.\nBuilding Pivot Tables After connecting our clean data to a new spreadsheet, I created several pivot tables to conduct some prescriptive analysis and to get a better idea of what the data looks like. First, I created a pivot table that describes the average ride length for casual users, subscribed members, and overall users. The pivot table shows that casual users ride for longer periods of time than member users. Next, I created two pivot tables to show the minimum and maximum ride lengths in 2022. These pivot tables show that our shortest ride was one minute, while our longest ride was close to ten hours!\nFor the next two pivot tables, I added another layer of depth. First, I wanted to see how user types’ average ride lengths varied per day. Thus, I created a pivot table that shows user type on rows, day of the week on columns, and average ride length in each cell. Finally, I created a pivot table that explores the same dimensions as the previous pivot table, but this time, looks at the total number of rides in each cell. The picture below shows what these pivot tables look like.\nConclusion This concludes the use of Microsoft Excel as a data analysis tool. Although the process and analyze stages were successful, they were not as effective as possible. This is because more suitable tools like SQL and R were not used. In the next section, I will show how to complete the previous two phases using SQL in MySQL Workbench, DBeaver, and Visual Studio Code for SQL.\nImage Credits Image Courtesy of: Wilfred Iven\n","permalink":"https://eangutierrez.github.io/portfolio/blog/part4/","summary":"Data Analysis Process Stage 4: Analyze Now that we have uploaded, cleaned, and processed our data, we are ready to analyze and find key insights to help Cyclistic better understand their customers. In the Analyze stage, we use tools to format and transform, sort, and filter the data to find patterns and draw conclusions. We can make predictions and recommendation to help our stakeholders make data-informed decisions.\nPower Query Editor With a validated and clean data set in hand, we are ready to start the data analysis stage.","title":"Cyclistic Bike-Share Business Case Part Four: Excel Analysis"},{"content":"Data Analysis Process Stage 4: Analyze Now that we have uploaded, cleaned, and processed our data, we are ready to analyze and find key insights to help Cyclistic better understand their customers. In the Analyze stage, we use tools to format and transform, sort, and filter the data to find patterns and draw conclusions. We can make predictions and recommendation to help our stakeholders make data-informed decisions.\nCreate a SQL Database In this section, we will carry out the previous data pre-processing task and data analysis steps using a more practical tool called Structured Query Language, or SQL. First, it is important to note that we went back to the raw, unprocessed data. We will run a code script to load all 12 CSV files into one SQL table.\nPreprocess Data We repeated lines 22 through 27 for the remaining CSV files to fill the table. Once the table contains all 2022 bike rental records, we ran the following script to tidy the data and prepare it for analysis.\nBenefits of SQL SQL is much more flexible than Excel, and we can easily hide irrelevant information. In Excel, we deleted the ride_id row because we wanted to protect our customers’ identity. With SQL, we can keep that data for query purposes and hide it from unauthorized users by creating a VIEW table. A view is a virtual table that is only stored in the memory and does not take up actual storage space. Let us further explore VIEWS.\nSQL VIEWS allow us to keep the minimum number of columns in storage space to 13 to improve system performance. VIEWS can also help us protect the main database from accidents and can improve data literacy by giving more employees access to the data. The script below creates a VIEW that contains our base query query. The results are shown below.\nJust like when using Power Query Editor, the ride_length column helped identify incoherent data records, such as trips with negative or zero-minute ride lengths. Without a client to discuss the best way to handle these records, I decided to delete those rows from our analysis.\nBetter Analysis with SQL Pivot Tables After creating our base query, my next step was to recreate the previously built spreadsheet pivot tables using SQL. The first table we created found the average ride length in minutes of casual users, members, and both groups combined.\nThe second table we created found the shortest bike ride in our data set, while the third showed the longest.\nThe fourth table we created found the average ride length for our three user groups and filtered them by the days of the week.\nThe fifth table we created found the number of rides for our three user groups and filtered them by the days of the week. The fourth and fifth tables help us find our user riding habits based on the day of the week.\nThe sixth table we created counts the total number of users and divided their rides into three categories: short rides with a ride length of less than 10 minutes, medium rides with a ride length of between 10 and 20 minutes, and long rides of more than 20 minutes.\nThe seventh table we created found the number of rides for our three user groups and filtered them out by the month of the year.\nThe eight table we created found the top 10 most common rides in Chicago that year.\nThe last table we created found the number of rides for our three user groups and filtered them out by the hour of the day.\nYou can find a copy of the entire MySQL script on my GitHub page.\nConclusion These nine pivot tables are a good starting point for creating a SQL report that we can send to our stakeholders. Although SQL is a fantastic tool for calculating, sorting, and filtering our data, it lacks the ability to graphically interpret our data. The report we have created is good, but it is hard for people to make sense of data by only looking at tables. Luckily, we can perform our analysis in RStudio, which not only is a fantastic tool to calculate, sort, and filter data, but it also has many visualization capabilities to better understand our data. In the next section, I will show how to complete the previous two phases using RStudio and the R programming language.\nImage Credits Image Courtesy of: Caspar Camille Rubin\n","permalink":"https://eangutierrez.github.io/portfolio/blog/part5/","summary":"Data Analysis Process Stage 4: Analyze Now that we have uploaded, cleaned, and processed our data, we are ready to analyze and find key insights to help Cyclistic better understand their customers. In the Analyze stage, we use tools to format and transform, sort, and filter the data to find patterns and draw conclusions. We can make predictions and recommendation to help our stakeholders make data-informed decisions.\nCreate a SQL Database In this section, we will carry out the previous data pre-processing task and data analysis steps using a more practical tool called Structured Query Language, or SQL.","title":"Cyclistic Bike-Share Business Case Part Five: SQL Analysis"},{"content":"Build RStudio Data Frame In this section, we will carry out the previous data pre-processing and data analysis steps using a more practical tool called the R programming language. First, it is important to note that we went back to the raw, unprocessed data. We will run a code script to load all 12 CSV files into one R data frame.\nPreprocess Data in R Once the data frame contains all 20222 bike rental records, we ran the following script to tidy the data and prepare it for analysis\nBenefits of R R is much faster than SQL because it loads the data on RAM instead of a hard disk. That means that we can add a column for the day of the week the ride took place, and the ride duration in minutes without losing performance.\nR is so powerful that we can easily recreate the previously built pivot tables using R code. Note that the numbers there is a tiny variation in the numbers depending on the tool that was used to analyze the data. This is because each tool has its own built logic when it approaches borderline cases. At this time, any data analyst would like to discuss these small discrepancies with the company’s data engineers and with the client to come up with a sensible solution. Because this is a capstone project and the client does not exist, we must take account of that phenomenon whenever it occurs in the real world. We will continue our analysis for now and further analyze the data.\nData Visualizations in R One of the most powerful aspects of R is that we can create powerful visualizations to convey useful facts about our data. It’s hard for people to understand the scale of our data, or discern key variables from looking at pivot tables. However, we can use that information to build striking visualizations that highlight what truly matters. For example, it can be intimidating to look at table seven, where we broke down the number of rides per user type per month. Which months have the highest number of rides? Are there seasons where the users choose other methods of transportation? And what type of users use the bikes more each month? Our pivot table has all of that information, but it is hard for management to find the answer. Let’s use that data to create a visualization that answers these questions.\nNumber of Rides for 2022 per User Group This is a powerful visualization because it easily shows answers to our questions. First, a big portion of our users rent bikes from May through October. This makes sense because we know that Chicago has wonderful climate on those months. Second, it seems that the usage rate of our bikes dramatically decreases on January, February, and December. It’s important to point out that our member users, in green, continue to ride at a higher rate than the casual users, in red. Third, the member users outperform the casual users throughout the entire year. The data shows that the member users rent the bikes more, so there is an incentive for Cyclistic to apply marketing strategies to convert casual users to member users, especially during peak riding season, to convert casual users to members and maximize profit.\nRide Distribution Over Two Categories Another powerful visualization is a variable tree that shows the data frame nested into subsets. Variable trees are useful to identify patterns in our data. Using the type of user on the first layer, our data is nested into two subsets: the casual user and the member user. This layer shows that 60% of our users are subscribers, and 40% of the users are casual users. Adding the type of bike as a second layer creates additional subsets. Of the member users, 66% use classic bikes, and 34% use electric bikes. Additionally, of the casual users, 51 % use classic bikes, 39% use electric bikes, and 10% use docked bikes. As we are dealing with a business case, we cannot raise concerns to our client, but in the real world, we should discuss with our clients whether the docked bike type should be changed to another type of bike, or whether it should be filtered from our analysis.\nWe can draw two conclusions from this visualization:\n· Member users do not use docked bikes.\n· Both casual and member users prefer classic bikes.\nDistribution of Rides per Minute We can also build a histogram to look at the frequency distribution of bike rides per ride length:\nWe can draw two conclusions from this visualization:\n· The vast majority of the rides have a ride length of less than 10 minutes.\n· Rides longer than ~ 25 minutes have a frequency of less than 50 K rides.\nBecause the majority of rides have a ride length of less than 10 minutes, it could mean that the bikes are rented with a specific objective in mind, such as commuting to work, or getting groceries, instead of riding for leisure.\nNumber of Rides per Day of the Week We can also build a bar graph that divides the rides per day of the week to find additional user preferences:\nWe can draw two conclusions from this visualization:\n· Member users use bikes throughout the entire week, but there is a decrease in use on weekends.\n· On weekends, casual users ride more. There are about + 100 K bike rides on Saturday, and about + 50 K more rides on Sunday.\nBecause there is an increase of casual users on weekends, Cyclistic Bike-Share could capitalize by running marketing promotions to target user conversion and promote user growth.\nNumber of Rides per Hour of the Day We can also build a heat map that looks at the day of the week and the hour of the day to find the times our users prefer to ride bikes:\nWe can draw several conclusions from this visualization:\n· From Tuesday through Friday, the most popular use of bikes is for commutes from 6:00 am to 8:00 am, and from 4:00 pm to 6:00 pm.\n· On Saturday, the most popular use of bikes is from the hours of 7:00 am to 8:00 am, and from 3:00 pm to 6:00 pm.\n· On Sunday, the most popular use of bikes is from the hours of 9:00 am to 6:00 pm.\nKnowing the most popular times of use is very useful for the marketing team. Cyclistic Bike-Share could capitalize by running marketing promotions to offer discounts in non-popular hours to promote bike usage, or increase the prices during popular hours if bike demand overtakes supply.\nBuild Pie Charts if Requested We can also build visualizations that compare usage among groups of users. Although there are better ways to visualize this information, some clients may prefer pie charts, and the team built this to be prepared in this case:\nWe can draw two conclusions from this visualization:\n· Both casual and member users rent bikes mainly on weekdays.\n· Casual users use bikes on weekends at a higher rate than member users.\nBuild Maps Showing Popular Routes Finally, we can create a visualization of a map that contains the most popular routes by both member and casual users:\nWe can draw several conclusions from these visualization:\n· Member users are more spread throughout the area.\n· Many casual riders start from the Chicago Harbor.\n· It seems that member users have shorter trips than casual users. This lines up with our previous assessment that casual users ride for leisure, while member users ride with a purpose.\nCyclistic Bike-Share can take advantage of this opportunity by placing ads to promote user conversion near the Chicago Harbor area, while also assessing proper inventory levels to keep up with demand.\nYou can find a copy of the entire R script on my GitHub page.\nConclusion These visualizations are a good starting point for creating a stakeholder presentation. Although R is an excellent data analysis and visualization tool, there are several organizations that choose to use other programming languages to analyze and visualize data. In the next section, I will show how to complete the previous two phases using Python and Jupyter Lab.\n","permalink":"https://eangutierrez.github.io/portfolio/blog/part6/","summary":"Build RStudio Data Frame In this section, we will carry out the previous data pre-processing and data analysis steps using a more practical tool called the R programming language. First, it is important to note that we went back to the raw, unprocessed data. We will run a code script to load all 12 CSV files into one R data frame.\nPreprocess Data in R Once the data frame contains all 20222 bike rental records, we ran the following script to tidy the data and prepare it for analysis","title":"Cyclistic Bike-Share Business Case Part Six: RStudio Analysis"},{"content":"Creating Python DataFrame In this section, we will carry out the previous data pre-processing and data analysis steps using a more popular language called Python. First, it is important to note that we went back to the raw, unprocessed data. We will run a code script to load all 12 CSV files into one data frame.\nPreprocessing Data in Python Once the data frame contains all bike rental records, we ran the following script to tidy the data and prepare it for analysis\nBenefits of Python Although R is much faster than Python because it loads the data on RAM instead of a hard disk, it also brings a few drawbacks. Because RAM is being used, data analysts must be mindful of how much memory is being used, as RAM is much scarcer than hard disk memory. With large datasets such as the one we are working on, we must account for all the variables we store in memory, all the different copies of the dataset as we pre-process the data, and the number of records being passed to a graphic visualization function.\nMemory usage in Python is much less of a concern. This is because it uses virtual memory from the computer’s operating system. This gives data analysts a lot of flexibility:\n· With proper nomenclature, data analysts can assign variable names to the dataset during each of the data pre-processing steps\n· At its core, Python is a general-purpose, object-oriented programming language that emphasizes code readability\n· This means that complex tasks, such as creating robust data visualizations, can be broken down into small, simple steps. This is vital when working in teams and reading another person’s work\nPython\u0026rsquo;s Strenghts Because of the reasons stated above, Python is one of the most popular programming languages today. Python users are not confined to just performing data analytics tasks. They can create computer programs, build and maintain data pipelines, and many other tasks. Many companies are partial to this flexibility, which is why good analysts must be able to write both R and Python code. The great news is that R provides a strong foundation to quickly transition to Python.\nThe code below demonstrates how Python can effortlessly produce the same pivot tables we built in Excel, SQL, and R.\nMotivation to Learn Python Each programming language has its own strengths and weaknesses. Naturally, there are some tasks that are much more difficult to complete in some languages than others. One of the main examples encountered during this business case is the R Vtree library. Python does not have an easy way to create a variable tree. Luckily, one of Python’s strengths is creating interactive visualizations using the Plotly library. You can see an example of an interactive graph called a Sankey Chart, which shows the distribution of all the records via through the user type and bicycle type categories.\nPython’s standard plotting library is called Matplotlib, and through it we can easily produce insightful visualizations, just like with R’s ggplot2 package.\nPython is constantly improving because of dedicated research teams who build and improve current libraries. For example, Seaborn uses Matplotlib as a base and builds an even higher-level interface to easily plot complicated graphs, such as heat maps.\nBecause more people are familiar with Python, it can be much easier to adapt techniques from other computer languages to Python. For example, the Folium library helps brings the incredible versatility of JavaScript’s leaflet.js library to plot interactive maps similar to those found on Tableau.\nYou can find a copy of the entire Python script on my GitHub page.\nConclusion As you can see, Python is a powerful tool that every data analyst should understand. In the real world, there are complex problems that are better approached with a particular language, or with a specific data library or package. The more tools at your disposal, the better. Although Python is good at creating visualizations, we can use data visualization tools like Tableau to create the most professional visualizations and tell a story through our data. In the next section, I will show how to create top-level visualizations in Tableau.\n","permalink":"https://eangutierrez.github.io/portfolio/blog/part7/","summary":"Creating Python DataFrame In this section, we will carry out the previous data pre-processing and data analysis steps using a more popular language called Python. First, it is important to note that we went back to the raw, unprocessed data. We will run a code script to load all 12 CSV files into one data frame.\nPreprocessing Data in Python Once the data frame contains all bike rental records, we ran the following script to tidy the data and prepare it for analysis","title":"Cyclistic Bike-Share Business Case Part Seven: Python"},{"content":"Data Analysis Process Stage 5: Share Now that the Analysis is complete, we must bring the data to life. It was hard to understand the data just by looking at pivot tables. This is why we used R and Python to create data visualizations that tell a story through the data. But showing cool visualizations to our stakeholders is not enough. We must bring context and communicate to help them understand the results of our analysis.\nThis is what The Data Analysis Process\u0026rsquo; Share Stage is all about. We must organize our visualizations into a coherent presentation, explain what each of these visualizations mean, and provide data-driven recommendations to help our stakeholders understand the differences between casual users and member users.\nTableau\u0026rsquo;s Strenghts Tableau is a professional tool used to create graphics and other professional visualizations to tell a story through data. Additionally, we can combine Tableau visualizations to create dashboards that easily convey complex ideas. These dashboards can be interactive to let the users change data filters or parameters to look at the data from multiple points of view. Finally, you can combine several dashboards to create story boards that answer questions by deep diving into the data.\nLet’s take a look at one of the visualizations (or Tableau Sheets) we created to show Tableau’s capabilities. Tableau’s version of the variable tree chart is the bubble chart. By adding the number of users as a measure and the user and bike types as dimensions, Tableau created the following graph:\nTo people that lack the business case’s context, this may be difficult to interpret. Thankfully, we can use Tableau’s Tooltip to provide additional information. The Tooltip is activated every time we hover our mouse on a specific object:\nHovering over the biggest bubble told us that this bubble represents the number of member users, the amount of member users, and the bike type. Here we also color-coded each bubble to represent casual users (in blue) and member users (in pink). Finally, the size of each bubble shows the number of records associated with each bike type. This is a strong visualization because we have incorporated multiple elements, size, color, and interactivity, to help users understand.\nThis additional context can be crucial to tactile learners. On a storyboard, we can even create a textbox that provides additional information to help anyone who uses our files:\nThe Power of Tableau Dashboards With Tableau, we can use color, shape, size, and other elements to create powerful visualizations such as this:\nThis is a Tableau dashboard. Dashboards are combinations of data visualizations that are combined to convey a complex idea. For example, this dashboard combines two visualizations: a map that shows a map that displays the most popular routes, and a horizontal stacked bar graph that shows the most popular routes and the number of times that route was taken. We can do several things with this dashboard. First, we can click on a specific route’s name or bar graph to display it on the map:\nWe can also provide filters to maximize or minimize the number of routes. We can seven provide additional context and instructions to help any user who may be intimidated by technology. This is a visualization without using a minimum or maximum number of times a route was used:\nYou can find a copy of the entire Tableau Story on my Tableau Public.\nConclusion With a completed storyboard, we are ready to deliver a thorough presentation to our client. After completing our data visualizations in Tableau, we are ready to move to the last stage of the data analysis process: the Act stage. In the next section, we will share our completed storyboard along with a set of recommendations with the Cyclistic Bike-Share leadership.\n","permalink":"https://eangutierrez.github.io/portfolio/blog/part8/","summary":"Data Analysis Process Stage 5: Share Now that the Analysis is complete, we must bring the data to life. It was hard to understand the data just by looking at pivot tables. This is why we used R and Python to create data visualizations that tell a story through the data. But showing cool visualizations to our stakeholders is not enough. We must bring context and communicate to help them understand the results of our analysis.","title":"Cyclistic Bike-Share Business Case Part Eight: Tableau Visualizations"},{"content":"Data Analysis Process Stage 6: Act The last step of the data analysis process is to act on the insights gained during the previous phases. Data analysts present their findings to leadership via PowerPoint presentations, Tableau Dashboards, or other methods. You can find a copy of the completed Tableau Storyboard we created by following this link to our Tableau Public Profile.\nStakeholder Recommendations Now that the clients have access to our Tableau Dashboard, it’s time to provide a set of recommendations based on our findings. These will guide leadership to make data-driven decisions and act on whatever the data shows. Based on the data, the data analytics team has the following set of recommendations:\nFirst, run advertising campaigns from May through October to convert casual users to members. Any advertising investment during peak season will maximize our product’s exposure to the public and promote customer conversions.\nSecond, carry out weekend promotions and events to target new users and inform them of membership benefits. The number of casual users dramatically increases on weekends. Therefore, weekend promotions will increase the audience size.\nThird, target member users to encourage renting bikes for recreational riding to boost profits. The data suggests that member users mostly see our products as a means to commute to school or work. Company profits will grow if we can get member users to use our bikes outside of commuting hours.\nAs a final recommendation, the data analytics team sees future research opportunities in researching Chicago neighborhood demographics. If we can compare these demographics to the neighborhoods where our bikes are most popular, we can determine what neighborhoods have the best conditions for our company to expand and gain more customers.\nWhat I Learned Here is a list of skills I learned throughout this project:\nMySQL server options Uploading data to database servers Data transformation and reporting in R and Python Tableau Story customization Website creation via Hugo Image Credits Image courtesy of: Viktor Keri\n","permalink":"https://eangutierrez.github.io/portfolio/blog/part9/","summary":"Data Analysis Process Stage 6: Act The last step of the data analysis process is to act on the insights gained during the previous phases. Data analysts present their findings to leadership via PowerPoint presentations, Tableau Dashboards, or other methods. You can find a copy of the completed Tableau Storyboard we created by following this link to our Tableau Public Profile.\nStakeholder Recommendations Now that the clients have access to our Tableau Dashboard, it’s time to provide a set of recommendations based on our findings.","title":"Cyclistic Bike-Share Business Case Part Nine: Stakeholder Recommendations"}]